{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd0y3h47Znl0rkCuoWK9zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pires1789/funcoes_etl_python/blob/main/funcoes_etl_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBSP_SFCsRga"
      },
      "outputs": [],
      "source": [
        "# Função 01\n",
        "# Carregar arquivos CSV\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_csv(file_path, delimiter=',', header=True, encoding='utf-8'):\n",
        "    if header:\n",
        "        df = pd.read_csv(file_path, delimiter=delimiter, encoding=encoding)\n",
        "    else:\n",
        "        df = pd.read_csv(file_path, delimiter=delimiter, header=None, encoding=encoding)\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# df = load_csv('caminho/para/arquivo.csv', encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função 02\n",
        "# Remover colunas e linhas com valores ausentes.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def drop_missing_values(df, threshold=0.5):\n",
        "    # Calcula o limite de valores não ausentes necessário\n",
        "    col_thresh = int(threshold * len(df))\n",
        "    row_thresh = int(threshold * len(df.columns))\n",
        "\n",
        "    # Remove colunas com muitos valores ausentes\n",
        "    df_cleaned = df.dropna(axis=1, thresh=col_thresh)\n",
        "\n",
        "    # Remove linhas com muitos valores ausentes\n",
        "    df_cleaned = df_cleaned.dropna(axis=0, thresh=row_thresh)\n",
        "\n",
        "    return df_cleaned"
      ],
      "metadata": {
        "id": "aI1xlVFMsr0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função 03\n",
        "# Padronizar nome de colunas\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def clean_column_names(df):\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "        .str.strip()  # Remove espaços em branco do início e fim\n",
        "        .str.lower()  # Converte para minúsculas\n",
        "        .str.replace(' ', '_')  # Substitui espaços por underscores\n",
        "        .str.replace(r'[^\\w]', '_', regex=True)  # Remove caracteres especiais\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# df = pd.DataFrame({'Nome Coluna': [1, 2], 'Outra@Coluna!': [3, 4]})\n",
        "# df = clean_column_names(df)"
      ],
      "metadata": {
        "id": "EAvKwLJVse8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função 04\n",
        "# Alterar o padrão de data.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def transform_date_column(df, column_name, format='%Y-%m-%d'):\n",
        "    df[column_name] = pd.to_datetime(df[column_name], errors='coerce').dt.strftime(format)\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# data = {'data': ['2023-01-01', '01/02/2023', 'Mar 3, 2023']}\n",
        "# df = pd.DataFrame(data)\n",
        "# df = transform_date_column(df, 'data', format='%d-%m-%Y')\n",
        "# print(df)\n"
      ],
      "metadata": {
        "id": "kCsSpWa_sV-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função 05\n",
        "# Agregação com base em colunas específicas.\n",
        "\"\"\"\n",
        "    Realiza agregação de dados com base em colunas específicas.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame a ser agregado.\n",
        "    group_by_columns (list): Lista de colunas para agrupar.\n",
        "    aggregation_dict (dict): Dicionário definindo as funções de agregação para cada coluna.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: DataFrame agregado com base nas colunas e funções especificadas.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def aggregate_data(df, group_by_columns, aggregation_dict):\n",
        "    aggregated_df = df.groupby(group_by_columns).agg(aggregation_dict).reset_index()\n",
        "    return aggregated_df"
      ],
      "metadata": {
        "id": "REDardCSuCiG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_outliers(df, column_name, method='IQR'):\n",
        "    \"\"\"\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame com os dados.\n",
        "    column_name (str): Nome da coluna a ser filtrada.\n",
        "    method (str): Método para identificar outliers (por exemplo, 'IQR', 'Z-score').\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: DataFrame com outliers removidos da coluna especificada.\n",
        "    \"\"\"\n",
        "    if method == 'IQR':\n",
        "        Q1 = df[column_name].quantile(0.25)\n",
        "        Q3 = df[column_name].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        filtered_df = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
        "    elif method == 'Z-score':\n",
        "        from scipy.stats import zscore\n",
        "        df['z_score'] = zscore(df[column_name])\n",
        "        filtered_df = df[(df['z_score'] >= -3) & (df['z_score'] <= 3)].drop(columns=['z_score'])\n",
        "    else:\n",
        "        raise ValueError(\"Método não suportado. Use 'IQR' ou 'Z-score'.\")\n",
        "\n",
        "    return filtered_df\n"
      ],
      "metadata": {
        "id": "i3VYWNTfuvyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def join_dataframes(df1, df2, on_columns, how='inner'):\n",
        "    \"\"\"\n",
        "    Realiza a junção de dois DataFrames com base em colunas comuns.\n",
        "\n",
        "    Parâmetros:\n",
        "    df1 (DataFrame): Primeiro DataFrame.\n",
        "    df2 (DataFrame): Segundo DataFrame.\n",
        "    on_columns (list): Lista de colunas para a junção.\n",
        "    how (str): Tipo de junção (por exemplo, 'inner', 'left', 'right', 'outer').\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: DataFrame resultante da junção.\n",
        "    \"\"\"\n",
        "    joined_df = pd.merge(df1, df2, on=on_columns, how=how)\n",
        "    return joined_df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# df1 = pd.DataFrame({'ID': [1, 2, 3], 'Nome': ['Alice', 'Bob', 'Charlie']})\n",
        "# df2 = pd.DataFrame({'ID': [1, 2, 4], 'Idade': [25, 30, 35]})\n",
        "# df_joined = join_dataframes(df1, df2, on_columns=['ID'], how='inner')\n",
        "# print(df_joined)\n"
      ],
      "metadata": {
        "id": "Naen9tnRvBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize_column(df, column_name):\n",
        "    \"\"\"\n",
        "    Normaliza os valores de uma coluna para uma faixa padrão (0 a 1).\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame com a coluna a ser normalizada.\n",
        "    column_name (str): Nome da coluna a ser normalizada.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: DataFrame com a coluna normalizada.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    df[column_name] = scaler.fit_transform(df[[column_name]])\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# data = {'Valores': [10, 12, 14, 15, 18, 19, 20]}\n",
        "# df = pd.DataFrame(data)\n",
        "# df_normalized = normalize_column(df, 'Valores')\n",
        "# print(df_normalized)\n"
      ],
      "metadata": {
        "id": "opDweer0vCbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_features(df, column_name, new_columns):\n",
        "    \"\"\"\n",
        "    Extrai características adicionais de uma coluna e as adiciona como novas colunas.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame contendo a coluna de origem.\n",
        "    column_name (str): Nome da coluna a partir da qual extrair características.\n",
        "    new_columns (list): Lista de novos nomes de colunas para as características extraídas.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame: DataFrame com as novas colunas adicionadas.\n",
        "    \"\"\"\n",
        "    for new_column in new_columns:\n",
        "        if new_column == 'year':\n",
        "            df['year'] = pd.to_datetime(df[column_name]).dt.year\n",
        "        elif new_column == 'month':\n",
        "            df['month'] = pd.to_datetime(df[column_name]).dt.month\n",
        "        elif new_column == 'day':\n",
        "            df['day'] = pd.to_datetime(df[column_name]).dt.day\n",
        "        elif new_column == 'weekday':\n",
        "            df['weekday'] = pd.to_datetime(df[column_name]).dt.weekday\n",
        "        elif new_column == 'hour':\n",
        "            df['hour'] = pd.to_datetime(df[column_name]).dt.hour\n",
        "        elif new_column == 'minute':\n",
        "            df['minute'] = pd.to_datetime(df[column_name]).dt.minute\n",
        "        elif new_column == 'second':\n",
        "            df['second'] = pd.to_datetime(df[column_name]).dt.second\n",
        "        else:\n",
        "            raise ValueError(f\"Característica '{new_column}' não é suportada.\")\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso:\n",
        "# data = {'data_hora': ['2023-01-01 12:00:00', '2023-01-02 13:30:00', '2023-01-03 15:45:00']}\n",
        "# df = pd.DataFrame(data)\n",
        "# df = extract_features(df, 'data_hora', ['year', 'month', 'day', 'hour', 'minute', 'second'])\n",
        "# print(df)\n"
      ],
      "metadata": {
        "id": "JwUa0mZSvExp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def save_to_csv(df, file_path, index=False):\n",
        "    \"\"\"\n",
        "    Salva um DataFrame em um arquivo CSV.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (DataFrame): DataFrame a ser salvo.\n",
        "    file_path (str): Caminho para o arquivo CSV de destino.\n",
        "    index (bool): Indica se o índice do DataFrame deve ser incluído no CSV.\n",
        "\n",
        "    Retorna:\n",
        "    None\n",
        "    \"\"\"\n",
        "    df.to_csv(file_path, index=index)\n",
        "\n",
        "# Exemplo de uso:\n",
        "# data = {'Nome': ['Alice', 'Bob', 'Charlie'], 'Idade': [25, 30, 35]}\n",
        "# df = pd.DataFrame(data)\n",
        "# save_to_csv(df, 'dados.csv')\n"
      ],
      "metadata": {
        "id": "ao50mhtsvM8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}